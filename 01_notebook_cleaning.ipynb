{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src = \"./resources/GA.png\" width = \"25\" height = \"25\" /> <span style = \"color:Purple\" > Project 5 : Food Insecurity Regression Study </span> \n",
    "---\n",
    "## <span style = \"color:Green\" > Preprocessing / Modeling </span>      \n",
    "\n",
    "#### Ira Seidman, Alec Edgecliffe-Johnson, Ryan McDonald, Andrew Roberts - General Assembly \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Graphing imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer #imputer imports\n",
    "from copy import copy, deepcopy #copy imports\n",
    "\n",
    "# Modeling imports for imputing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh = pd.read_csv('data_inputs/ed_socio_health.csv')\n",
    "df_wp = pd.read_csv('data_inputs/wage_poverty.csv')\n",
    "df_un = pd.read_csv('data_inputs/unemployment_clean.csv')\n",
    "df_fins = pd.read_csv('data_inputs/food_ins_18.csv')\n",
    "df_ed = pd.read_csv('data_inputs/education_stats_dsi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh.fips.astype(int)\n",
    "df_wp.fips.astype(int)\n",
    "df_un.fips.astype(int)\n",
    "df_fins.fips.astype(int)\n",
    "df_ed.fips.astype(int).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Dataframes\n",
    "Merge each dataset into one main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.merge(left = df_sh, right = df_wp, on = 'fips')\n",
    "df_m = pd.merge(left = df_m, right = df_un, on = 'fips')\n",
    "df_m = pd.merge(left = df_m, right = df_fins, on = 'fips')\n",
    "df_m = pd.merge(left = df_m, right = df_ed, on = 'fips' )\n",
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming state_x as full_st_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_m.rename(columns = {\"state_x\": \"state_name\",\n",
    "                               \"state_y\": \"state_abr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns that are unlikely to have explanatory power over and above other variables and that are duplicate information from 2016 data (eg. 2019 unemployment data).\n",
    "\n",
    "Dropping num and percent food insecure in 2016 dataset as the 2018 data has both children and total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f584e3c77bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_m' is not defined"
     ]
    }
   ],
   "source": [
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping list of columns that would appear unlikely to be the strongest predictors of food insecurity and have a fair amount of nulls\n",
    "drop_list = ['teen_birth_rate', 'age_adjusted_death_rate', 'child_mortality_rate', \n",
    "             'infant_mortality_rate', 'num_limited_access_to_healthy_foods', \n",
    "             'segregation_index', 'segregation_index_2', 'homicide_rate', \n",
    "             'suicide_rate_age_adjusted', 'juvenile_arrest_rate', 'area_name', \n",
    "             'num_below_poverty', 'percent_some_college', 'labor_force', \n",
    "             'percent_unemployed_CHR', 'med_inc_19', 'unemployment_rate_2019', \n",
    "             'med_household_inc_19', 'med_hh_income_percent_of_state_total_2019', \n",
    "             'num_food_insecure', 'percent_food_insecure', 'less_than_high_school_diploma', \n",
    "             'bachelor_degree_or_higher', 'percent_less_than_18_years_of_age', 'percent_65_and_over',\n",
    "             'mental_health_provider_rate']\n",
    "\n",
    "df_m = df_m.drop(columns = drop_list)\n",
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop % and convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m['fi_rate_18'] = df_m['fi_rate_18'].str.replace('%', '').astype(float)\n",
    "df_m['ch_fi_rate_18'] = df_m['ch_fi_rate_18'].str.replace('%', '').astype(float)\n",
    "df_m['cpm_18'] = df_m['cpm_18'].str.strip('US$').astype(float)\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check types\n",
    "types = pd.DataFrame(df_m.dtypes)\n",
    "types.rename(columns = {0 : 'type'}, inplace = True)\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values - much less than the previous 9405\n",
    "df_m.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = [col for col in df_m if df_m[col].isna().any()]\n",
    "\n",
    "# No need for dummies on the first three dfs because a simple imputation is being used without ml models that need numerical data\n",
    "df_m_mean = deepcopy(df_m)\n",
    "df_m_median = deepcopy(df_m)\n",
    "df_m_mode = deepcopy(df_m)\n",
    "df_m_knn = pd.get_dummies(deepcopy(df_m))\n",
    "df_m_lr = pd.get_dummies(deepcopy(df_m))\n",
    "df_m_rf = pd.get_dummies(deepcopy(df_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with mean, median, and mode\n",
    "for col in null_columns:\n",
    "    df_m_mean[col] = df_m[col].fillna(df_m[col].dropna().mean())\n",
    "    df_m_median[col] = df_m[col].fillna(df_m[col].dropna().median())\n",
    "    df_m_mode[col] = df_m[col].fillna(df_m[col].dropna().mode()[0])\n",
    "    \n",
    "print('Mean imputation nulls: ', df_m_mean.isnull().sum().sum())\n",
    "print('Median imputation nulls: ', df_m_median.isnull().sum().sum())\n",
    "print('Mode imputation nulls: ', df_m_mode.isnull().sum().sum())\n",
    "\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "def impute_missing_data(model):\n",
    "    if model == lr:\n",
    "        df = deepcopy(df_m_lr)\n",
    "    elif model == rf:\n",
    "        df = deepcopy(df_m_rf)\n",
    "    # Loop through each column that has null values to impute for each row in that column with predictions from model\n",
    "    for col in null_columns:\n",
    "        df_cc = df.dropna() #use complete case\n",
    "        \n",
    "        # Fit model\n",
    "        X = df_cc.drop(columns = col)\n",
    "        y = df_cc[col]\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        df_temp = deepcopy(df) #deep copy to avoid making update\n",
    "        \n",
    "        # Fillna temporarily for other columns with median - eventually all columns will be imputed with model, but in the meantime impute columns left to be imputed with the median\n",
    "        for column in df_temp.columns:\n",
    "            if column != col:\n",
    "                df_temp[column] = df_temp[column].fillna(df_temp[column].dropna().median())\n",
    "                \n",
    "        X_temp = df_temp.drop(columns = col) #drop target for prediction so there is no nulls\n",
    "\n",
    "        # Loop through all of the rows checking for nulls in the col column, create a pred, and set that cell equal to pred\n",
    "        for index, row in df_temp.iterrows():\n",
    "            if pd.isnull(df_temp[col].iloc[index]):\n",
    "                X_test_row = X_temp.iloc[index] #use df without target\n",
    "                X_test_row = X_test_row.values.reshape(1, -1)\n",
    "                \n",
    "                pred = model.predict(X_test_row)\n",
    "                df_temp.loc[index, col] = pred\n",
    "                #print(pred[0])\n",
    "        df[col] = df_temp[col] #make updates to df for next loop\n",
    "    return df\n",
    "\n",
    "df_m_lr = impute_missing_data(lr)\n",
    "print('Lr imputation nulls: ', df_m_lr.isnull().sum().sum())\n",
    "\n",
    "df_m_rf = impute_missing_data(rf)\n",
    "print('Rf imputation nulls: ',df_m_rf.isnull().sum().sum())\n",
    "\n",
    "imp_knn = KNNImputer(n_neighbors = 2)\n",
    "df_m_knn = imp_knn.fit_transform(df_m_knn)\n",
    "df_m_knn = pd.DataFrame(df_m_knn, columns = list(df_m_rf.columns))\n",
    "print('Knn imputation nulls: ', df_m_knn.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_mean.to_csv('./cleaned_dataframes/df_m_mean.csv', index = False)\n",
    "df_m_median.to_csv('./cleaned_dataframes/df_m_median.csv', index = False)\n",
    "df_m_mode.to_csv('./cleaned_dataframes/df_m_mode.csv', index = False)\n",
    "df_m_knn.to_csv('./cleaned_dataframes/df_m_knn.csv', index = False)\n",
    "df_m_lr.to_csv('./cleaned_dataframes/df_m_lr.csv', index = False)\n",
    "df_m_rf.to_csv('./cleaned_dataframes/df_m_rf.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
